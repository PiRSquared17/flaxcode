
Crowdsourcing is a process that involves outsourcing tasks to a distributed group of people. This process can occur both online and offline, and the difference between crowdsourcing and ordinary outsourcing is that a task or problem is outsourced to an undefined public rather than a specific body, such as paid employees. 

Crowdsourcing is related to, but not the same as, human-based computation, which refers to the ways in which humans and computers can work together to solve problems. These two methods can be used together to accomplish tasks.

Some have questioned the ethical validity of providing no compensation or small amounts of compensation to members of the crowd that contribute to tasks.

==Definitions==
Crowdsourcing is a distributed problem-solving and production model. In the classic use of the term, problems are broadcast to an unknown group of solvers in the form of an open call for solutions. Users—also known as the crowd—submit solutions. Solutions are then owned by the entity that broadcast the problem in the first place—the crowdsourcer. The contributor of the solution is, in some cases, compensated either monetarily, with prizes, or with recognition. In other cases, the only rewards may be kudos or intellectual satisfaction. Crowdsourcing may produce solutions from amateurs or volunteers working in their spare time, or from experts or small businesses which were unknown to the initiating organization.

Those who use crowdsourcing services, also known as crowdsourcers, are motivated by the benefits of crowdsourcing, which are that they can gather large numbers of solutions or information and that it is relatively inexpensive to obtain this work. Users are motivated to contribute to crowdsourced tasks by both intrinsic motivations, such as social contact and passing the time, and by extrinsic motivations, such as financial gain.

Due to the blurred limits of crowdsourcing, many collaborative activities, online or not, are being considered crowdsourcing when they are not. Another consequence of this situation is the proliferation of definitions in the scientific literature. Different authors give different definitions of crowdsourcing according to their specialities, losing in this way the global picture of the term.

Estellés and González (2012), after studying more than 40 definitions of crowdsourcing, propose a new integrating definition: 

==History==
Crowdsourcing systems are used to accomplish a variety of tasks. For example, the crowd may be invited to develop a new technology, carry out a design task (also known as community-based design or distributed participatory design), refine or carry out the steps of an algorithm (see human-based computation), or help capture, systematize, or analyze large amounts of data (see also citizen science).

The term "crowdsourcing" is a portmanteau of "crowd" and "outsourcing," coined by Jeff Howe in a June 2006 "Wired" magazine article "The Rise of Crowdsourcing".

===Historical examples===

Long before modern crowdsourcing systems were developed, there were a number of notable examples of projects that utilized distributed people to help accomplish tasks. 
====Early Crowdsourcing====
The Oxford English Dictionary (OED) may provide one of the earliest examples of crowdsourcing. An open call was made to the community for contributions by volunteers to index all words in the English language and example quotations for each and every one of their usages. They received over 6 million submissions over a period of 70 years. The making of the OED is detailed in The Surgeon of Crowthorne by Simon Winchester. 
====Early Crowdsourcing Competitions====
Similarly, crowdsourcing has often been used in the past as a competition in order to discover a solution. The French government proposed several of these competitions, often rewarded with Montyon Prizes, created for poor Frenchmen who had done virtuous acts. These included the Leblanc process, or the Alkali Prize, where a reward was provided for separating the salt from the alkali, and the Fourneyron's Turbine, when the first hydraulic commercial turbine was developed. 

In response to a challenge from the French government, Nicholas Appert won a prize for inventing a new way of food preservation that involved sealing food in air-tight jars. The British government provided a similar reward to find an easy way to determine a ship’s longitude in the The Longitude Prize. During the Great Depression, out-of-work clerks tabulated higher mathematical functions in the Mathematical Tables Project as an outreach project.

In 1994, Northeast Consulting compiled a database of trends in the marketplace. This database was collected from numerous sources, offering an example of early crowdsourcing.

==Modern methods==
Today, crowdsourcing has transferred mainly to the web. The web provides a particularly good venue for crowdsourcing since individuals tend to be more open in web-based projects where they are not being physically judged or scrutinized and thus can feel more comfortable sharing. This ultimately allows for well-designed artistic projects because individuals are less conscious, or maybe even less aware, of scrutiny towards their work. In an online atmosphere more attention is given to the project rather than communication with other individuals. 

Crowdsourcing can either take an explicit or an implicit route. Explicit crowdsourcing lets users work together to evaluate, share, and build different specific tasks, while implicit crowdsourcing means that users solve a problem as a side effect of something else they are doing.

With explicit crowdsourcing, users can evaluate particular items like books or webpages, or share by posting products or items. Users can also build artifacts by providing information and editing other people's work.

Implicit crowdsourcing can take two forms: standalone and piggyback. Standalone allows people to solve problems as a side effect of the task they are actually doing, whereas piggyback takes users' information from a third-party website to gather information.

===Types of crowdsourcing===

In coining the term of "crowdsourcing", Jeff Howe has also indicated some common categories of crowdsourcing that can be used effectively in the commercial world. Some of these web-based crowdsourcing efforts include crowdvoting, wisdom of the crowd, crowdfunding, microwork, and inducement prize contests. Although these may not be an exhaustive list, they cover the current major ways in which people use crowds to perform tasks.

====Crowdvoting====

Crowdvoting occurs when a website gathers a large group's opinions and judgment on a certain topic. The Iowa Electronic Market is a prediction market that gathers crowds' views on politics and tries to ensure accuracy by having participants pay money to buy and sell contracts based on political outcomes.

 Threadless.com selects the t-shirts it sells by having users provide designs and vote on the ones they like, which are then printed and available for purchase. Despite the small nature of the company, thousands of members provide designs and vote on them, making the website’s products truly created and selected by the crowd, rather than the company. Some of the best examples have been through social media channels where big brands like Domino's Pizza, Coca Cola, Heineken and Sam Adams have crowdsourced a new pizza, song, bottle design and beer respectively.

iStockPhoto provides a platform for people to upload photos and purchase them for low prices. Clients can purchase photos through credits, giving photographers a small profit. Again, the photo collection is determined by the crowd's voice for very low prices. 

===="Wisdom of the crowd"====

 Wisdom of the crowd is another type of crowdsourcing that collects large amounts of information and aggregates it to gain a complete and accurate picture of a topic, based on the idea that a group of people is often more intelligent than an individual. This idea of collective intelligence proves particularly effective on the web because people can contribute in real-time within the same forums from very diverse backgrounds. 

=====Recent Examples of "Wisdom of the crowd"=====
The wisdom of the crowd has become increasingly common. The 2011 documentary "Life In a Day" used this type of crowdsourcing to collect video footage from people worldwide. Contributors from 192 countries submitted 4,500 hours of video, primarily through YouTube uploads, which the filmmakers edited to 97 minutes in order to create a cohesive documentary. 

Harvard Tuberculosis Lab teamed with CrowdFlower to help identify drug resistant TB cells in mouse cortex slides. Employing wisdom of the crowd allowed the project to be completed efficiently by being able to expand beyond their limited employee base.

In February 2012, a stock picking game called "Ticker Picker Pro" was launched, using crowdsourcing to create a hedge fund that would buy and sell stocks based on the ideas coming out of the game. These crowdsourced ideas, coming from so many people, could help one pick the best stocks based on this idea that collective ideas are better than individual ones.

====Crowdfunding====

Crowdfunding is the process of funding your projects by a multitude of people contributing a small amount in order to attain a certain monetary goal. These people are often recruited from social networks, where the funds can be acquired from an equity purchase, loan, donation, or pre-ordering. The amounts collected have become quite high, with requests that are over a million dollars for software like Trampoline Systems, which used it to finance the commercialization of their new software. 

A well-known crowdfunding tool is Kickstarter, which is the biggest website for funding creative projects. It has raised over $100 million, despite its all-or-nothing model which requires one to reach the proposed monetary goal in order to acquire the money. Crowdrise brings together volunteers to fundraise in an online environment.

====Microwork====

Microwork is a crowdsourcing platform where users do small tasks for which computers lack aptitude for low amounts of money. Amazon’s popular Mechanical Turk has created many different projects for users to participate in, where each task requires very little time and offers a very small amount in payment. The Chinese versions of this, commonly called Witkeys, are similar and include such sites as Taskcn.com and k68.cn. When choosing tasks, since only certain users “win”, users learn to submit later and pick less popular tasks in order to increase the likelihood of getting their work chosen. An example of a Mechanical Turk project is when users searched satellite images for images of a boat in order to find lost researcher Jim Gray.

====Inducement prize contests====

Web-based idea competitions, or inducement prize contests often consist of generic ideas, cash prizes, and an Internet-based platform to facilitate easy idea generation and discussion. An example of these competitions includes an event like IBM’s 2006 “Innovation Jam”, attended by over 140,000 international participants and yielding around 46,000 ideas. 

Another example of competition-based crowdsourcing is the 2009 DARPA experiment, where DARPA placed 10 balloon markers across the United States and challenged teams to compete to be the first to report the location of all the balloons. A collaboration of efforts was required to complete the challenge quickly and in addition to the competitive motivation of the contest as a whole, the winning team (MIT, in less than nine hours) established its own "collaborapetitive" environment to generate participation in their team.

Open innovation platforms are a very effective way of crowdsourcing people’s thoughts and ideas to do research and development. The company InnoCentive is a crowdsourcing platform for corporate research and development where difficult scientific problems are posted for crowds of solvers to discover the answer and win a cash prize, which can range from $10,000 to $100,000 per challenge. IdeaConnection.com challenges people to come up with new inventions and innovations and Ninesigma.com connects clients with experts in various fields. The X PRIZE Foundation creates and runs incentive competitions where one can win between $1 million and $30 million for solving challenges.

====Implicit crowdsourcing====

Implicit crowdsourcing is less obvious because users do not necessarily know they are contributing, yet can still be very effective in completing certain tasks. Rather than users actively participating in solving a problem or providing information, implicit crowdsourcing involves users doing another task entirely where a third party gains information for another topic based on the user’s actions.

A good example of implicit crowdsourcing is the ESP game, where users guess what images are and then these labels are used to tag Google images. Another popular use of implicit crowdsourcing is through reCAPTCHA, which asks people to solve Captchas in order to prove they are human, and then provides Captchas from old books that cannot be deciphered by computers in order to try and digitize them for the web. Like Mechanical Turk, this task is simple for humans but would be incredibly difficult for computers.

Piggyback crowdsourcing can be seen most frequently by websites such as Google that mine one’s search history and websites in order to discover keywords for ads, spelling corrections, and finding synonyms. In this way, users are unintentionally helping to modify existing systems, such as Google’s ad words.

==Crowdsourcers==

There are a number of motivations for businesses to use crowdsourcing to accomplish tasks, find solutions for problems, or to gather information. These include the ability to offload peak demand, access cheap labor and information, generate better results, access a wider array of talent than might be present in one organization, and undertake problems that would have been too difficult to solve internally. Crowdsourcing allows businesses to submit problems in which contributors can work on, such as problems in science, manufacturing, biotech, and medicine, with monetary rewards for successful solutions. Although it can be difficult to crowdsource complicated tasks, simple work tasks can be crowdsourced cheaply and effectively.

Crowdsourcing also has the potential to be a problem-solving mechanism for government and nonprofit use. Urban and transit planning are prime areas for crowdsourcing. One project to test crowdsourcing's public participation process for transit planning in Salt Lake City has been underway from 2008 to 2009, funded by a U.S. Federal Transit Administration grant. Another notable application of crowdsourcing to government problem solving is the Peer to Patent Community Patent Review project for the U.S. Patent and Trademark Office. Crowdsourcing is also considered a tool for innovation cooperation across prosperity gaps and therefore a way of moderating side effects of global brain drains and workforce migration. 

Researchers have used crowdsourcing systems, in particular Mechanical Turk, to aid with research projects by crowdsourcing aspects of the research process such as data collection, parsing, and evaluation. Notable examples include using the crowd to create speech and language databases , and using the crowd to conduct user studies . Crowdsourcing systems provide these researchers with the ability to gather large amount of data. Additionally, using crowdsourcing, researchers can collect data from populations and demographics they may not have had access to locally, but that improve the validity and value of their work. 

Artists have also utilized crowdsourcing systems. In his project the Sheep Market, Aaron Koblin used Mechanical Turk to collect 10,000 drawings of sheep from contributors around the world.. Sam Brown (artist) leverages the crowd by asking visitors of his website explodingdog to send him sentences that he uses as inspirations for paintings. Art curator Andrea Grover argues that individuals tend to be more open in crowdsourced projects because they are not being physically judged or scrutinized. As with other crowdsourcers, artists use crowdsourcing systems to generate and collect data. The crowd also can be used to provide inspiration for an artist’s work and also to collect financial support for that work. 

==Demographics==

The crowd is an umbrella term for people who contribute to crowdsourcing efforts. Though it is sometimes difficult to gather data about the demographics of the crowd, a study by Ross et al surveyed the demographics of a sample of the more than 400,000 registered crowdworkers using Amazon Mechanical Turk to complete tasks for pay. While a previous study in 2008 by Ipeirotis found that users at that time were primarily American, young, female, and well-educated, with 40% having incomes >$40,000/yr, in 2009 Ross found a very different population. By Nov. 2009, 36% of the surveyed Mechanical Turk workforce was Indian. ⅔ of Indian workers were male, and 66% had at least a Bachelor’s degree. ⅔ had annual incomes less than $10,000/yr, with 27% sometimes or always depending on income from Mechanical Turk to make ends meet.

The average US user of Mechanical Turk earned $2.30 per hour for tasks in 2009, versus $1.58 for the average Indian worker. While the majority of users worked less than 5 hours per week, 18% worked 15 hours per week or more. This is less than minimum wage in either country, which Ross suggests raises ethical questions for researchers who use crowdsourcing.

The demographics of http://microworkers.com/ differ from Mechanical Turk in that the US and India together account for only 25% of workers. 197 countries are represented among users, with Indonesia (18%) and Bangladesh (17%) contributing the largest share. However, 28% of employers are from the US.

==Motivations==

Kaufmann and Schulze suggest that there are both intrinsic and extrinsic motivations that cause people to contribute to crowdsourced tasks, and that these factors influence different types of contributors. For example, students and people employed full-time rate Human Capital Advancement as less important than part-time workers do, while woman rate Social Contact as more important than men do.

Intrinsic motivations are broken down into two categories, enjoyment-based and community-based motivations. Enjoyment-based motivations refer to motivations related to the fun and enjoyment that the contributor experiences through their participation. These motivations include: skill variety, task identity, task autonomy, direct feedback from the job, and pastime. Community-based motivations refer to motivations related to community participation, and include community identification and social contact.

Extrinsic motivations are broken down into three categories, immediate payoffs, delayed payoffs, and social motivations. Immediate payoffs, through monetary payment, are the immediately received compensations given to those who complete tasks. Delayed payoffs are benefits that can be used to generate future advantages, such as training skills and being noticed by potential employers. Social motivations are the rewards of behaving pro-socially, such as altruistic motivations. Chandler and Kapelner found that US users of the Amazon Mechanical Turk were more likely to complete a task when told they were going to “help researchers identify tumor cells,” than when they were not told the purpose of their task. However, of those who completed the task, quality of output did not depend on the framing of the task.

Another form of social motivation is prestige or status. The International Children's Digital Library recruits volunteers to translate and review books. Because all translators receive public acknowledgment for their contribution, Kaufman and Schulz cite this as a reputation-based strategy to motivate individuals who want to be associated with institutions that have prestige. The Amazon Mechanical Turk uses reputation as a motivator in a different sense, as a form of quality control. Crowdworkers who frequently complete tasks in ways judged to be inadequate can be denied access to future tasks, providing motivation to produce high-quality work.

==Criticisms==
There are two major categories of criticisms about crowdsourcing, (1) the value and impact of the work received from the crowd and (2) the ethical implications of low wages paid to crowdworkers. Most of these criticisms are directed towards crowdsourcing systems that provide extrinsic monetary rewards to contributors, though some apply more generally to all crowdsourcing systems.

===Concerns for crowdsourcers===
Concerns for crowdsources include:

Susceptibility to faulty results caused by targeted, malicious work efforts. Since crowdworkers completing microtasks are paid per task, there is often a financial incentive to complete tasks quickly rather than well. Verifying responses is time consuming, and so requesters often depend on having multiple workers complete the same task to correct errors. However, having each task completed multiple times increases time and monetary costs.

Crowdworkers are a nonrandom sample of the population. Many researchers use crowdsourcing in order to quickly and cheaply conduct studies with larger sample sizes than would be otherwise achievable. However, due to low worker pay, participant pools are skewed towards poor users in developing countries.

Ethical concerns. Because crowdworkers are considered independent contractors rather than employees, they are not guaranteed a minimum wage. In practice, workers using the Amazon Mechanical Turk generally earn less than the minimum wage, even in India. Some researchers considering using Mechanical Turk to get participants for studies have argued that this may be unethical.

Increased likelihood that a crowdsourced project will fail due to lack of monetary motivation or too few participants. Crowdsourcing markets are not a first-in-first-out queue. Tasks that are not completed quickly may be forgotten, buried by filters and search procedures so that workers do not see them. This results in a long tail power law distribution of completion times. Additionally, low-paying research studies online have higher rates of attrition, with participants not completing the study once started. Even when tasks are completed, crowdsourcing doesn't always produce quality results. When Facebook began its localization program in 2008, it encountered criticism for the low quality of its crowdsourced translations.

===Concerns for users of crowdsourcing platforms===
Concerns for members of the crowd include:

Below-market wages. The average US user of Mechanical Turk earned $2.30 per hour for tasks in 2009, versus $1.58 for the average Indian worker. While the majority of users worked less than 5 hours per week, 18% worked 15 hours per week or more, and 27% of Indian users said income from Mechanical Turk is sometimes or always necessary for them to make ends meet. This is less than minimum wage in either country, which Ross et al. suggest raises ethical questions for researchers who use crowdsourcing. When Facebook began its localization program in 2008, it received criticism for using crowdsourcing to obtain free labor.

No written contracts, non-disclosure agreements, or employee agreements or agreeable terms with crowdsourced employees. For users of the Amazon Mechanical Turk, this means that requestors have final say over whether users’ work is acceptable; if not, they will not be paid.

Difficulties in collaboration of crowd members, especially in the context of competitive crowd sourcing. Crowdsourcing site InnoCentive allows organizations to solicit solutions to scientific and technological problems; only 10.6% of respondents report working in a team on their submission.

==See Also==
* Citizen science
* Clickworkers
* Collective intelligence
* Collaborative innovation network
* Crowdcasting
* Distributed thinking
* Flash mob
* Human-based computation
* List of crowdsourcing projects
* Open innovation
* Open-source autopilot hardware and software 
* Smart mob
* Social collaboration
* Wisdom of the crowd
==References==













